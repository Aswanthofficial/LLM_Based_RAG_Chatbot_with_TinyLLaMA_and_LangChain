# LLM_Based_RAG_Chatbot_with_TinyLLaMA_and_LangChain

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using:

- ğŸ§  **TinyLLaMA** (1.1B) for local question answering
- ğŸ”— **LangChain** to orchestrate the retrieval + generation workflow
- ğŸ§¾ **FAISS** for semantic document retrieval
- ğŸ“š **HuggingFace Sentence Embeddings** to vectorize input text
- ğŸ“„ Text file loader and chunking for document ingestion

It enables local, private, and efficient **document-based QA** â€” ideal for notebooks, reports, or offline AI assistants.

---

## ğŸ“Œ Key Technologies

| Component          | Tool/Library                            |
|--------------------|-----------------------------------------|
| LLM                | TinyLLaMA (via `llama-cpp-python`)      |
| Embeddings         | HuggingFace (`all-MiniLM-L6-v2`)        |
| Vector Database    | FAISS                                   |
| Retrieval Pipeline | LangChain `RetrievalQA`                 |
| Document Loader    | LangChain `TextLoader`                  |

---

## ğŸš€ Features

- âœ… Load and split `.txt` files into vectorized chunks
- âœ… Generate embeddings using SentenceTransformers
- âœ… Store and search vectors using FAISS
- âœ… Use a lightweight local LLM (TinyLLaMA) for generation
- âœ… Fully local, offline-compatible RAG pipeline

---

## ğŸ› ï¸ Installation

Install dependencies:

```bash
pip install -q langchain llama-cpp-python faiss-cpu sentence-transformers
pip install langchain_community
